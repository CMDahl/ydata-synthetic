# -*- coding: utf-8 -*-
"""Kopi af TimeGAN - Synthetic stock data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1jfvmpFKNTB-EnhfwbW7CSTHp0iV4m--5
"""

# Note: You can select between running the Notebook on "CPU" or "GPU"
# Click "Runtime > Change Runtime time" and set "GPU"

#Uncomment to install ydata-synthetic lib
#! pip install ydata-synthetic

"""# Time Series synthetic data generation with TimeGAN

- TimeGAN - Implemented accordingly with the [paper](https://papers.nips.cc/paper/8789-time-series-generative-adversarial-networks)
- This notebook is an example of how TimeGan can be used to generate synthetic time-series data.

## Dataset and imports

- The data used in this notebook was downloaded from [Yahoo finance](https://finance.yahoo.com/quote/GOOG/history?p=GOOG) and includes:
    - **6 variables** - Open, High, Low, Close, Adj Close, Volume
    - **1022 events** registered between the period of 1 January 2017 - 24 January 2021.
    - The data was processed using a MinMaxScaler (all the variables were numeric)
"""

#Importing the required libs for the exercise

from os import path
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf



from sklearn.preprocessing import MinMaxScaler


from ydata_synthetic.synthesizers import ModelParameters
from ydata_synthetic.preprocessing.timeseries import processed_stock
from ydata_synthetic.synthesizers.timeseries import TimeGAN
from sklearn.model_selection import TimeSeriesSplit

def fit_ar_1(ts):
    try:
        x = ts[0:-1]
        y = ts[1:]
        p = np.polyfit(x.reshape(-1), y.reshape(-1), 1)
        beta = p[0]

        c = np.mean(ts) * (1 - beta)
        sigma = 0;
    except:
        beta = 0;
        c = 0;
        sigma = 0;

    return beta, c, sigma


def simulate_ar_1(alpha, T, warm = 1000):
    innovations = np.random.normal(size = T + warm);

    # this is retarded, find out how to use a filter instead
    y = [];
    for i in range(len(innovations)):
        if i == 0:
            y.append(innovations[i]);
            continue;

        y.append(y[i - 1] * alpha + innovations[i]);

    return y[warm:];


class Blocked_Series_Dataset():
    def __init__(self, series, block_size = 100):
        super(Blocked_Series_Dataset, self).__init__();

        self.series = series;
        self.block_size = block_size;
        self.blocks = self._create_blocks(self.series);

    def _create_blocks(self, series):
        blocks = [];
        n = len(series);
        for i in range(n - self.block_size):
            blocks.append((series[i:i + self.block_size]));

        return blocks;
    
    def _create_blocks_split(self, series):
        blocks = [];
        n = len(series);
        splits  = np.floor(n/self.block_size)
        d1 = seq_len*(np.int32(splits))
        d2 =  np.int32(splits)
        blocks = np.split(d[:d1],d2)
        blocks= [i.reshape((-1,1)) for i in blocks]
        return blocks;

    def __len__(self):
        return len(self.blocks)

# Method implemented here: https://github.com/jsyoon0823/TimeGAN/blob/master/data_loading.py
# Originally used in TimeGAN research
def real_data_loading(data: np.array, seq_len):
    """Load and preprocess real-world datasets.
    Args:
      - data_name: Numpy array with the values from a a Dataset
      - seq_len: sequence length

    Returns:
      - data: preprocessed data.
    """
    # Flip the data to make chronological data
    ori_data = data[::-1]
    # Normalize the data
    scaler = MinMaxScaler().fit(ori_data)
    ori_data = scaler.transform(ori_data)

    # Preprocess the dataset
    temp_data = []
    # Cut data by sequence length
    for i in range(0, len(ori_data) - seq_len):
        _x = ori_data[i:i + seq_len]
        temp_data.append(_x)

    # Mix the datasets (to make it similar to i.i.d)
    idx = np.random.permutation(len(temp_data))
    data = []
    for i in range(len(temp_data)):
        data.append(temp_data[idx[i]])
    return data
    

# d = tf.random.normal(
#    (1000,1), mean=0.0, stddev=1.0, dtype=tf.dtypes.float32, seed=None, name=None
# ).numpy()


# splits = np.int(np.floor(len(d)/3))

"""## Define Model hyperparameters

**Networks:**
- Generator
- Discriminator
- Embedder
- Recovery Network

TimeGAN is a Generative model based on RNN networks. In this package the implemented version follows a very simple architecture that is shared by the four elements of the GAN.

Similarly to other parameters, the architectures of each element should be optimized and tailored to the data.
"""

#Specific to TimeGANs
seq_len=150
#n_seq = 6
#seq_len=150
n_seq = 1


hidden_dim=24
gamma=1

noise_dim = 32
dim = 128
batch_size = 32

log_step = 100
learning_rate = 5e-4

gan_args = ModelParameters(batch_size=batch_size,
                           lr=learning_rate,
                           noise_dim=noise_dim,
                           layers_dim=dim)

y  = np.array(simulate_ar_1(0.5, T=1000, warm = 1000)).reshape(-1,1)
ar05 = real_data_loading(y,seq_len)

#ar05 = Blocked_Series_Dataset(y,block_size=seq_len) 
#ar05 = gendata._create_blocks_split(y)   
#ar05 = [i.reshape((-1,1)) for i in ar05]
"""## The data"""

#stock_data = processed_stock(path='Z:/faellesmappe/cmd/tfs_alt/ydata-synthetic/data/stock_data.csv', seq_len=seq_len)
#print(len(stock_data),stock_data[0].shape)

"""## Training the TimeGAN synthetizer"""

if path.exists('synthesizer_stock.pkl'):
    synth = TimeGAN.load('synthesizer_stock.pkl')
else:
    synth = TimeGAN(model_parameters=gan_args, hidden_dim=24, seq_len=seq_len, n_seq=n_seq, gamma=1)
    #synth.train(stock_data, train_steps=5000)
    synth.train(ar05, train_steps=10)
    synth.save('synthesizer_stock.pkl')

"""### The generated synthetic stock data"""
estimates = [];
for i in range(samples.shape[0]):
    sample = samples[i, :];

    try:
        alpha, _, _ = fit_ar_1(sample);
        estimates.append(alpha);
    except np.linalg.LinAlgError:
        print('warning: ar1 fit convergence error.');
        alpha, _, _ = (0, 0, 0);

# report metrics for the generative technique.
self.update_metric('g_a', np.mean(estimates));
self.update_metric('g_sd', np.std(estimates));

self.update_metric('g_025', np.quantile(estimates, 0.025));
self.update_metric('g_975', np.quantile(estimates, 0.975));

# log a bunch of quantiles
self.update_metric('g_05', np.quantile(estimates, 0.05));
self.update_metric('g_95', np.quantile(estimates, 0.95));

self.update_metric('g_10', np.quantile(estimates, 0.10));
self.update_metric('g_90', np.quantile(estimates, 0.90));

self.update_metric('g_20', np.quantile(estimates, 0.20));
self.update_metric('g_80', np.quantile(estimates, 0.80));


#synth_data = synth.sample(len(stock_data))
synth_data = synth.sample(1)
print(synth_data.shape)

fit_ar_1(ar05[0])
fit_ar_1(synth_data[0])

#Reshaping the data
cols = ['AR1']

#Plotting some generated samples. Both Synthetic and Original data are still standartized with values between [0,1]
fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(15, 10))

time = list(range(1,25))
obs = np.random.randint(1000)

for j, col in enumerate(cols):
    df = pd.DataFrame({'Real': ar05[10][:, j],
                   'Synthetic': synth_data[10][:, j]})
    df.plot(title = col,
            secondary_y='Synthetic data', style=['-', '--'])
fig.tight_layout()

"""#### Evaluation of the generated synthetic data (PCA and TSNE)"""

from sklearn.decomposition import PCA
from sklearn.manifold import TSNE

sample_size = 250
idx = np.random.permutation(len(stock_data))[:sample_size]

real_sample = np.asarray(stock_data)[idx]
synthetic_sample = np.asarray(synth_data)[idx]

#for the purpose of comparision we need the data to be 2-Dimensional. For that reason we are going to use only two componentes for both the PCA and TSNE.
stock_data_reduced = real_sample.reshape(-1, seq_len)
synth_data_reduced= np.asarray(synthetic_sample).reshape(-1,seq_len)

n_components = 2
pca = PCA(n_components=n_components)
tsne = TSNE(n_components=n_components, n_iter=300)

#The fit of the methods must be done only using the real sequential data
pca.fit(stock_data_reduced)

pca_real = pd.DataFrame(pca.transform(stock_data_reduced))
pca_synth = pd.DataFrame(pca.transform(synth_data_reduced))

data_reduced = np.concatenate((stock_data_reduced, synth_data_reduced), axis=0)
tsne_results = pd.DataFrame(tsne.fit_transform(data_reduced))

#The scatter plots for PCA and TSNE methods
import matplotlib.gridspec as gridspec
fig = plt.figure(constrained_layout=True, figsize=(20,10))
spec = gridspec.GridSpec(ncols=2, nrows=1, figure=fig)

#TSNE scatter plot
ax = fig.add_subplot(spec[0,0])
ax.set_title('PCA results',
             fontsize=20,
             color='red',
             pad=10)

#PCA scatter plot
plt.scatter(pca_real.iloc[:, 0].values, pca_real.iloc[:,1].values,
            c='black', alpha=0.2, label='Original')
plt.scatter(pca_synth.iloc[:,0], pca_synth.iloc[:,1],
            c='red', alpha=0.2, label='Synthetic')
ax.legend()

ax2 = fig.add_subplot(spec[0,1])
ax2.set_title('TSNE results',
              fontsize=20,
              color='red',
              pad=10)

plt.scatter(tsne_results.iloc[:sample_size, 0].values, tsne_results.iloc[:sample_size,1].values,
            c='black', alpha=0.2, label='Original')
plt.scatter(tsne_results.iloc[sample_size:,0], tsne_results.iloc[sample_size:,1],
            c='red', alpha=0.2, label='Synthetic')

ax2.legend()

fig.suptitle('Validating synthetic vs real data diversity and distributions',
             fontsize=16,
             color='grey')

"""#### Train synthetic test real (TSTR)"""

from tensorflow.keras import Input, Sequential
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.layers import GRU, Dense
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanAbsoluteError

#First implement a simple RNN model for prediction
def RNN_regression(units):
    opt = Adam(name='AdamOpt')
    loss = MeanAbsoluteError(name='MAE')
    model = Sequential()
    model.add(GRU(units=units,
                  name=f'RNN_1'))
    model.add(Dense(units=6,
                    activation='sigmoid',
                    name='OUT'))
    model.compile(optimizer=opt, loss=loss)
    return model

#Prepare the dataset for the regression model
stock_data=np.asarray(stock_data)
synth_data = synth_data[:len(stock_data)]
n_events = len(stock_data)

#Split data on train and test
idx = np.arange(n_events)
n_train = int(.75*n_events)
train_idx = idx[:n_train]
test_idx = idx[n_train:]

#Define the X for synthetic and real data
X_stock_train = stock_data[train_idx, :seq_len-1, :]
X_synth_train = synth_data[train_idx, :seq_len-1, :]

X_stock_test = stock_data[test_idx, :seq_len-1, :]
y_stock_test = stock_data[test_idx, -1, :]

#Define the y for synthetic and real datasets
y_stock_train = stock_data[train_idx, -1, :]
y_synth_train = synth_data[train_idx, -1, :]

print('Synthetic X train: {}'.format(X_synth_train.shape))
print('Real X train: {}'.format(X_stock_train.shape))

print('Synthetic y train: {}'.format(y_synth_train.shape))
print('Real y train: {}'.format(y_stock_train.shape))

print('Real X test: {}'.format(X_stock_test.shape))
print('Real y test: {}'.format(y_stock_test.shape))

#Training the model with the real train data
ts_real = RNN_regression(12)
early_stopping = EarlyStopping(monitor='val_loss')

real_train = ts_real.fit(x=X_stock_train,
                          y=y_stock_train,
                          validation_data=(X_stock_test, y_stock_test),
                          epochs=200,
                          batch_size=128,
                          callbacks=[early_stopping])

#Training the model with the synthetic data
ts_synth = RNN_regression(12)
synth_train = ts_synth.fit(x=X_synth_train,
                          y=y_synth_train,
                          validation_data=(X_stock_test, y_stock_test),
                          epochs=200,
                          batch_size=128,
                          callbacks=[early_stopping])

#Summarize the metrics here as a pandas dataframe
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_log_error
real_predictions = ts_real.predict(X_stock_test)
synth_predictions = ts_synth.predict(X_stock_test)

metrics_dict = {'r2': [r2_score(y_stock_test, real_predictions),
                       r2_score(y_stock_test, synth_predictions)],
                'MAE': [mean_absolute_error(y_stock_test, real_predictions),
                        mean_absolute_error(y_stock_test, synth_predictions)],
                'MRLE': [mean_squared_log_error(y_stock_test, real_predictions),
                         mean_squared_log_error(y_stock_test, synth_predictions)]}

results = pd.DataFrame(metrics_dict, index=['Real', 'Synthetic'])

results